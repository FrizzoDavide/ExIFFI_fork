{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PERFORMANCE METRICS \n",
    "\n",
    "Compute the performance metrics of the EIF and EIF+ models (for the moment let's consider just these two). \n",
    "\n",
    "Performance Metrics computed: \n",
    "\n",
    "1. The typical classification metrics that we can obtain with sklearn.metrics.classification_report\n",
    "\n",
    "2. The Average Precision -> this is obtained with sklearn.metrics.average_precision_score but we can still use the mean value obtained in the Average_Precision.ipynb notebook (the ones used to create the Violin Plot)\n",
    "\n",
    "3. The ROC AUC Score -> obtainable with the sklearn.metrics.auc_roc_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "import sklearn\n",
    "sys.path.append('../')\n",
    "from utils import *\n",
    "#from utils.feature_selection import *\n",
    "from plot import *\n",
    "from simulation_setup import *\n",
    "from models import *\n",
    "from models.forests import *\n",
    "from models.Extended_IF import *\n",
    "from models.Extended_DIFFI import *\n",
    "from models.Extended_DIFFI_original import *\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,average_precision_score,roc_auc_score\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import os\n",
    "import pickle \n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pkl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\lemeda98\\\\Desktop\\\\PHD Information Engineering\\\\ExIFFI\\\\ExIFFI\\\\data\\\\diffi_data'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('c:\\\\Users\\\\lemeda98\\\\Desktop\\\\PHD Information Engineering\\\\ExIFFI\\\\ExIFFI\\\\data\\\\diffi_data')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_read = open(os.getcwd()+'\\\\anomalies.pkl', \"rb\")\n",
    "loaded_dictionary = pickle.load(file_to_read)\n",
    "X_xaxis,X_yaxis,X_bisect,X_bisect_3d,X_bisect_6d=loaded_dictionary['X_xaxis'],loaded_dictionary['X_yaxis'],loaded_dictionary['X_bisec'],loaded_dictionary['X_bisec_3d'],loaded_dictionary['X_bisec_6d']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Report Table Computation Function\n",
    "\n",
    "The following functions can be found in the Python Script called performance_report_functions.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def if_predict(score,p):\n",
    "    y=score>np.sort(score)[::-1][int(p*len(score))]\n",
    "    return y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_if(y,score):\n",
    "    p=sum(y)/len(y)\n",
    "    y_pred=if_predict(score,p)\n",
    "    d={}\n",
    "    d['Precision']=sklearn.metrics.precision_score(y,y_pred) \n",
    "    d['Recall']=sklearn.metrics.recall_score(y,y_pred) \n",
    "    d['f1 score']=sklearn.metrics.f1_score(y,y_pred) \n",
    "    d['Accuracy']=sklearn.metrics.accuracy_score(y,y_pred) \n",
    "    d['Balanced Accuracy']=sklearn.metrics.balanced_accuracy_score(y,y_pred) \n",
    "    d['Average Precision']=sklearn.metrics.average_precision_score(y,y_pred) \n",
    "    d['ROC AUC Score']=sklearn.metrics.roc_auc_score(y,y_pred) \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_eif(y,score,X_test,model):\n",
    "    p=sum(y)/len(y)\n",
    "    y_pred=model._predict(X_test,p).astype(int)\n",
    "    d={}\n",
    "    d['Precision']=sklearn.metrics.precision_score(y,y_pred) \n",
    "    d['Recall']=sklearn.metrics.recall_score(y,y_pred)\n",
    "    d['f1 score']=sklearn.metrics.f1_score(y,y_pred)\n",
    "    d['Accuracy']=sklearn.metrics.accuracy_score(y,y_pred)\n",
    "    d['Balanced Accuracy']=sklearn.metrics.balanced_accuracy_score(y,y_pred)\n",
    "    d['Average Precision']=sklearn.metrics.average_precision_score(y,score)\n",
    "    d['ROC AUC Score']=sklearn.metrics.roc_auc_score(y,score)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(X_train,X_test,y):\n",
    "    \n",
    "    EIF=ExtendedIsolationForest(n_estimators=300,plus=0)\n",
    "    EIF.fit(X_train)\n",
    "\n",
    "    EIF_plus=ExtendedIsolationForest(n_estimators=300,plus=1)\n",
    "    EIF_plus.fit(X_train)\n",
    "\n",
    "    IF=IsolationForest(n_estimators=300,max_samples=min(len(X_train),256))\n",
    "    IF.fit(X_train)\n",
    "\n",
    "    score_if=-1*IF.score_samples(X_test)+0.5\n",
    "    score_eif=EIF.predict(X_test)\n",
    "    score_eif_plus=EIF_plus.predict(X_test)\n",
    "\n",
    "    metrics_if=performance_if(y,score_if)\n",
    "    metrics_eif=performance_eif(y,score_eif,X_test,EIF)\n",
    "    metrics_eif_plus=performance_eif(y,score_eif_plus,X_test,EIF_plus)\n",
    "\n",
    "    return metrics_if,metrics_eif,metrics_eif_plus\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_performance(metrics_dict,name,X_train,X_test,y):\n",
    "    metrics_dict[name]={}\n",
    "    metrics_dict[name][\"IF\"]={}\n",
    "    metrics_dict[name][\"EIF\"]={}\n",
    "    metrics_dict[name][\"EIF_plus\"]={}\n",
    "    metric_names=['Precision', 'Recall', 'f1 score', 'Accuracy', 'Balanced Accuracy', 'Average Precision', 'ROC AUC Score']\n",
    "\n",
    "    for metric_name in metric_names:\n",
    "        metrics_dict[name]['IF'][metric_name]=[]\n",
    "        metrics_dict[name]['EIF'][metric_name]=[]\n",
    "        metrics_dict[name]['EIF_plus'][metric_name]=[]\n",
    "\n",
    "\n",
    "    for i in tqdm(range(10)):\n",
    "        metrics_if,metrics_eif,metrics_eif_plus=evaluate_performance(X_train,X_test,y)\n",
    "\n",
    "        for metric_name in metric_names:\n",
    "            metrics_dict[name]['IF'][metric_name].append(metrics_if[metric_name])\n",
    "            metrics_dict[name]['EIF'][metric_name].append(metrics_eif[metric_name])\n",
    "            metrics_dict[name]['EIF_plus'][metric_name].append(metrics_eif_plus[metric_name])\n",
    "\n",
    "    for metric_name in metric_names:\n",
    "        metrics_dict[name]['IF'][metric_name+'_avg']=np.mean(np.array(metrics_dict[name]['IF'][metric_name]))\n",
    "        metrics_dict[name]['EIF'][metric_name+'_avg']=np.mean(np.array(metrics_dict[name]['EIF'][metric_name]))\n",
    "        metrics_dict[name]['EIF_plus'][metric_name+'_avg']=np.mean(np.array(metrics_dict[name]['EIF_plus'][metric_name]))\n",
    "     \n",
    "    \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOMATIC PERFORMANCE REPORT COMPUTATION FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_report(name,metrics_dict,metrics_dict_split):\n",
    "    \n",
    "    os.chdir('c:\\\\Users\\\\lemeda98\\\\Desktop\\\\PHD Information Engineering\\\\ExIFFI\\\\ExIFFI\\\\data')\n",
    "    if name=='diabetes' or name=='moodify':\n",
    "        X,y=csv_dataset(name,os.getcwd()+'\\\\')\n",
    "    else:\n",
    "        X,y=dataset(name,os.getcwd()+'\\\\')\n",
    "\n",
    "    X,y=downsample(X,y)\n",
    "    X_train,X_test=partition_data(X,y)\n",
    "    scaler=StandardScaler()\n",
    "    X_train=scaler.fit_transform(X_train)\n",
    "    X_test=scaler.transform(X_test)\n",
    "    y_train=np.zeros(X_train.shape[0])\n",
    "    y_test=np.ones(X_test.shape[0])\n",
    "    y=np.concatenate([y_train,y_test])\n",
    "    X_test=np.r_[X_train,X_test]\n",
    "    scaler2=StandardScaler()\n",
    "    X=scaler2.fit_transform(X)\n",
    "\n",
    "    #Compute Performance Report Table without split\n",
    "    metrics_dict=collect_performance(metrics_dict,name,X,X,y)\n",
    "\n",
    "    #Compute Performance Report Table with split\n",
    "    metrics_dict_split=collect_performance(metrics_dict_split,name,X_train,X_test,y)\n",
    "\n",
    "    print('--------------------------------------------------------')\n",
    "    print(name)\n",
    "    print()\n",
    "    print('f1 score and average precision no train test split ')\n",
    "    print(f'IF -> f1 score: {metrics_dict[name][\"IF\"][\"f1 score\"]}\\naverage precision: {metrics_dict[name][\"IF\"][\"Average Precision\"]}')\n",
    "    print(f'EIF -> f1 score: {metrics_dict[name][\"EIF\"][\"f1 score\"]}\\naverage precision: {metrics_dict[name][\"EIF\"][\"Average Precision\"]}')\n",
    "    print(f'EIF_plus -> f1 score: {metrics_dict[name][\"EIF_plus\"][\"f1 score\"]}\\naverage precision: {metrics_dict[name][\"EIF_plus\"][\"Average Precision\"]}')\n",
    "    print(' ')\n",
    "    print('f1 score and average precision with train test split ')\n",
    "    print(f'IF -> f1 score: {metrics_dict_split[name][\"IF\"][\"f1 score\"]}\\naverage precision: {metrics_dict_split[name][\"IF\"][\"Average Precision\"]}')\n",
    "    print(f'EIF -> f1 score: {metrics_dict_split[name][\"EIF\"][\"f1 score\"]}\\naverage precision: {metrics_dict_split[name][\"EIF\"][\"Average Precision\"]}')\n",
    "    print(f'EIF_plus -> f1 score: {metrics_dict_split[name][\"EIF_plus\"][\"f1 score\"]}\\naverage precision: {metrics_dict_split[name][\"EIF_plus\"][\"Average Precision\"]}')\n",
    "    print('-----------------------------------------------------------')\n",
    "\n",
    "    return metrics_dict,metrics_dict_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_report_synt(name,X_train,X_test,metrics_dict,metrics_dict_split):\n",
    "    \n",
    "    X=np.r_[X_train,X_test]\n",
    "    scaler=StandardScaler()\n",
    "    X_train=scaler.fit_transform(X_train)\n",
    "    X_test=scaler.transform(X_test)\n",
    "    y_train=np.zeros(X_train.shape[0])\n",
    "    y_test=np.ones(X_test.shape[0])\n",
    "    y=np.concatenate([y_train,y_test])\n",
    "    X_test=np.r_[X_train,X_test]\n",
    "    scaler2=StandardScaler()\n",
    "    X=scaler2.fit_transform(X)\n",
    "\n",
    "    #Compute Performance Report Table without split\n",
    "    metrics_dict=collect_performance(metrics_dict,name,X,X,y)\n",
    "\n",
    "    #Compute Performance Report Table with split\n",
    "    metrics_dict_split=collect_performance(metrics_dict_split,name,X_train,X_test,y)\n",
    "\n",
    "    print('--------------------------------------------------------')\n",
    "    print(name)\n",
    "    print()\n",
    "    print('f1 score and average precision no train test split ')\n",
    "    print(f'IF -> f1 score: {metrics_dict[name][\"IF\"][\"f1 score_avg\"]}\\naverage precision: {metrics_dict[name][\"IF\"][\"Average Precision_avg\"]}')\n",
    "    print(f'EIF -> f1 score: {metrics_dict[name][\"EIF\"][\"f1 score_avg\"]}\\naverage precision: {metrics_dict[name][\"EIF\"][\"Average Precision_avg\"]}')\n",
    "    print(f'EIF_plus -> f1 score: {metrics_dict[name][\"EIF_plus\"][\"f1 score_avg\"]}\\naverage precision: {metrics_dict[name][\"EIF_plus\"][\"Average Precision_avg\"]}')\n",
    "    print(' ')\n",
    "    print('f1 score and average precision with train test split ')\n",
    "    print(f'IF -> f1 score: {metrics_dict_split[name][\"IF\"][\"f1 score_avg\"]}\\naverage precision: {metrics_dict_split[name][\"IF\"][\"Average Precision_avg\"]}')\n",
    "    print(f'EIF -> f1 score: {metrics_dict_split[name][\"EIF\"][\"f1 score_avg\"]}\\naverage precision: {metrics_dict_split[name][\"EIF\"][\"Average Precision_avg\"]}')\n",
    "    print(f'EIF_plus -> f1 score: {metrics_dict_split[name][\"EIF_plus\"][\"f1 score_avg\"]}\\naverage precision: {metrics_dict_split[name][\"EIF_plus\"][\"Average Precision_avg\"]}')\n",
    "    print('-----------------------------------------------------------')\n",
    "\n",
    "    return metrics_dict,metrics_dict_split\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict={}\n",
    "metrics_dict_split={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re load the X_train dataset for each different synthetic dataset to avoid having it scaled multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_read = open(os.getcwd()+'\\\\ball_6_dim.pkl', \"rb\")\n",
    "loaded_dictionary = pickle.load(file_to_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synthetic Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_dict,metrics_dict_split=performance_report_synt('Xaxis',X_train,X_xaxis,metrics_dict,metrics_dict_split)\n",
    "X_train=loaded_dictionary['X_train']\n",
    "metrics_dict,metrics_dict_split=performance_report_synt('Yaxis',X_train,X_yaxis,metrics_dict,metrics_dict_split)\n",
    "X_train=loaded_dictionary['X_train']\n",
    "metrics_dict,metrics_dict_split=performance_report_synt('Bisect',X_train,X_bisect,metrics_dict,metrics_dict_split)\n",
    "X_train=loaded_dictionary['X_train']\n",
    "metrics_dict,metrics_dict_split=performance_report_synt('Bisect_3d',X_train,X_bisect_3d,metrics_dict,metrics_dict_split)\n",
    "X_train=loaded_dictionary['X_train']\n",
    "metrics_dict,metrics_dict_split=performance_report_synt('Bisect_6d',X_train,X_bisect_6d,metrics_dict,metrics_dict_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real World Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names=['wine','annthyroid','breastw','shuttle','pima','cardio','glass',\n",
    "             'ionosphere','pendigits','diabetes','moodify']\n",
    "for name in dataset_names:\n",
    "    metrics_dict,metrics_dict_split=performance_report(name,metrics_dict,metrics_dict_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save in pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\lemeda98\\\\Desktop\\\\PHD Information Engineering\\\\ExIFFI\\\\ExIFFI\\\\results\\\\davide\\\\Performance Report'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('c:\\\\Users\\\\lemeda98\\\\Desktop\\\\PHD Information Engineering\\\\ExIFFI\\\\ExIFFI\\\\results\\\\davide\\\\Performance Report')\n",
    "path=os.getcwd()\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = path + '\\\\Performance_Report_final_synt.pkl'\n",
    "with open(path, 'wb') as f:\n",
    "    pickle.dump(metrics_dict_split,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('c:\\\\Users\\\\lemeda98\\\\Desktop\\\\PHD Information Engineering\\\\ExIFFI\\\\ExIFFI')\n",
    "path = os.getcwd() + '\\\\results\\\\davide\\\\Performance Report\\\\Performance_Report_final_synt.pkl'\n",
    "with open(path, 'rb') as f:\n",
    "    Performance_report_synt = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
